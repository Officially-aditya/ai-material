\chapter{Classical Machine Learning: Thinking in Features}

\section{The Crux}
Neural networks get all the hype, but most production ML is still ``classical'' methods: linear models, decision trees, ensembles. Why? They're interpretable, debuggable, and often work better with small data. This chapter is about thinking in features, not layers.

\section{Why Linear Models Still Dominate Industry}

Walk into any real ML deployment, and you'll find:
\begin{itemize}
    \item Banks: Logistic regression for credit scores
    \item Ad platforms: Linear models for click prediction
    \item Fraud detection: Gradient boosted trees
\end{itemize}

Why not deep learning everywhere?

\subsection{Reason \#1: Interpretability}

Regulators, auditors, and customers ask: ``Why was this decision made?''

\textbf{Linear model}: ``Income weighted 0.3, debt ratio weighted -0.5, result was 0.7 > threshold.''

\textbf{Neural network}: ``Uh, 50 million parameters multiplied through 20 layers produced 0.7.''

Guess which one the bank's legal team approves?

\subsection{Reason \#2: Sample Efficiency}

Deep learning needs massive data. 10,000 examples? A neural net will overfit. A regularized linear model will generalize.

\begin{asidebox}
\textbf{Rule of thumb}: $<$100k examples? Try classical ML first.
\end{asidebox}

\subsection{Reason \#3: Debugging}

When a linear model fails:
\begin{itemize}
    \item Check feature distributions
    \item Look at coefficients
    \item Test on slices
\end{itemize}

When a neural net fails:
\begin{itemize}
    \item Â¯\textbackslash\_(ãƒ„)\_/Â¯
    \item Check everything
    \item Pray
\end{itemize}

\subsection{Reason \#4: Speed}

Linear model prediction: microseconds.

Neural network prediction: milliseconds (or worse).

At scale, milliseconds matter. Ad auctions, fraud detection, recommendation servingâ€”latency is money.

\section{The Core Idea: Features Are Everything}

Classical ML is about \textbf{feature engineering}: transforming raw data into representations that make patterns obvious.

\subsection{An Example}

Predicting house prices from \texttt{[bedrooms, sqft, zipcode]}.

\textbf{Bad features}:
\begin{lstlisting}[language=Python]
X = [bedrooms, sqft, zipcode]
\end{lstlisting}

Zipcode is a number like 94103. But arithmetic on zipcodes is meaningless. 94103 + 1 $\neq$ similar neighborhood.

\textbf{Better features}:
\begin{lstlisting}[language=Python]
X = [
    bedrooms,
    sqft,
    bedrooms * sqft,  # interaction
    log(sqft),  # diminishing returns on size
    is_zipcode_94103,  # one-hot encode zipcode
    is_zipcode_94104,
    ...
]
\end{lstlisting}

Now the model can capture:
\begin{itemize}
    \item Large houses aren't linearly more expensive (log transform)
    \item 4-bedroom mansions vs 4-bedroom shacks (interaction terms)
    \item Neighborhood effects (one-hot zipcodes)
\end{itemize}

\textbf{The Lesson}: Most of the intelligence is in feature engineering, not model complexity.

\subsection{The Dirty Secret}

Deep learning automates feature engineering. Instead of hand-crafting features, you let the network learn them. But if you have domain knowledge, hand-crafted features often beat learned onesâ€”especially with limited data.

\section{Bias-Variance Tradeoff: The Central Dogma}

This is the most important concept in ML.

\subsection{The Setup}

Your model makes errors. Those errors come from two sources:

\textbf{Bias}: The model is too simple to capture the pattern.

\textbf{Variance}: The model is too sensitive to training data noise.

\subsection{An Intuition}

Imagine you're shooting arrows at a target.

\textbf{High bias, low variance}: All arrows cluster together, but far from the bullseye. You're consistently wrong.

\textbf{Low bias, high variance}: Arrows are scattered all over. Sometimes you hit the bullseye, sometimes you miss wildly. You're inconsistently right.

\textbf{The Goal}: Low bias AND low variance. Arrows cluster on the bullseye.

\subsection{In ML Terms}

\textbf{High bias model}: Linear model trying to fit a curved pattern. Underfits. High training error, high test error.

\textbf{High variance model}: 100-degree polynomial fit to 10 data points. Overfits. Low training error, high test error.

\textbf{Just right}: Regularized model. Captures signal, ignores noise. Low training error, low test error.

\subsection{The Tradeoff}

Reducing bias (more complex model) increases variance.

Reducing variance (simpler model) increases bias.

You can't eliminate both. You balance them.

\subsection{How to Balance}

\begin{enumerate}
    \item \textbf{Start simple}: Linear model, shallow tree
    \item \textbf{Evaluate}: Does it underfit (high bias)? Overfit (high variance)?
    \item \textbf{Adjust}:
    \begin{itemize}
        \item Underfitting? Add complexity (more features, deeper model)
        \item Overfitting? Add regularization, reduce features, get more data
    \end{itemize}
\end{enumerate}

\section{Regularization: Punishing Complexity}

The core idea: don't just minimize error. Minimize error \textit{and} model complexity.

\subsection{L2 Regularization (Ridge)}

Add penalty for large weights:

\begin{equation}
\text{Loss} = \text{Error} + \lambda \cdot (\text{sum of squared weights})
\end{equation}

\textbf{Effect}: Weights shrink toward zero. Model becomes smoother, less prone to overfitting.

\textbf{Intuition}: ``I'll accept a bit more training error if it means my model generalizes better.''

\subsection{L1 Regularization (Lasso)}

\begin{equation}
\text{Loss} = \text{Error} + \lambda \cdot (\text{sum of absolute weights})
\end{equation}

\textbf{Effect}: Some weights go exactly to zero. You get \textbf{feature selection}â€”unimportant features are ignored.

\textbf{When to use}: Many features, you suspect most are irrelevant.

\subsection{The $\lambda$ Parameter}

$\lambda$ controls the bias-variance tradeoff:
\begin{itemize}
    \item $\lambda = 0$: No regularization. High variance.
    \item $\lambda = \infty$: Weights forced to zero. High bias.
    \item $\lambda = $ just right: Goldilocks zone.
\end{itemize}

Finding the right $\lambda$ is model selection (via cross-validation).

\subsection{The Mathematics of Regularization: Why It Works}

Regularization isn't just a heuristicâ€”it has deep mathematical foundations. This section rigorously derives why penalizing weights improves generalization.

\textbf{The Fundamental Problem}: Given training data $\{(x_1, y_1), \ldots, (x_n, y_n)\}$, find weights $\theta$ that minimize:

\begin{equation}
L(\theta) = \sum_{i} \text{loss}(f(x_i; \theta), y_i)
\end{equation}

But minimizing training loss alone leads to overfitting. We need to balance fit and simplicity.

\subsubsection{L2 Regularization (Ridge): Mathematical Derivation}

\textbf{Objective}:
\begin{align}
L_{\text{Ridge}}(\theta) &= \sum_{i} (y_i - \theta^\top x_i)^2 + \lambda||\theta||^2 \\
&= (y - X\theta)^\top(y - X\theta) + \lambda\theta^\top\theta
\end{align}

where $X \in \mathbb{R}^{n \times d}$ is the data matrix, $y \in \mathbb{R}^n$ is the label vector.

\textbf{Finding the optimal $\theta$}:

Take the gradient and set to zero:
\begin{align}
\nabla_\theta L_{\text{Ridge}} &= -2X^\top(y - X\theta) + 2\lambda\theta = 0 \\
X^\top X\theta + \lambda\theta &= X^\top y \\
(X^\top X + \lambda I)\theta &= X^\top y \\
\theta_{\text{ridge}} &= (X^\top X + \lambda I)^{-1} X^\top y
\end{align}

Compare to ordinary least squares (OLS):
\begin{equation}
\theta_{\text{ols}} = (X^\top X)^{-1} X^\top y
\end{equation}

\textbf{The $\lambda I$ term matters}:

\begin{enumerate}
    \item \textbf{Invertibility}: If $X^\top X$ is singular (more features than samples, or collinear features), it's not invertible. Adding $\lambda I$ makes $(X^\top X + \lambda I)$ positive definite $\rightarrow$ always invertible.

    \item \textbf{Shrinkage}: The solution shrinks toward zero.
\end{enumerate}

\textbf{Proof of shrinkage} (via SVD):

Decompose $X = U\Sigma V^\top$ (singular value decomposition).

OLS solution:
\begin{equation}
\theta_{\text{ols}} = V\Sigma^{-1}U^\top y
\end{equation}

Ridge solution:
\begin{equation}
\theta_{\text{ridge}} = V(\Sigma^2 + \lambda I)^{-1}\Sigma U^\top y
\end{equation}

For singular value $\sigma_i$:
\begin{itemize}
    \item OLS coefficient scaled by $1/\sigma_i$
    \item Ridge coefficient scaled by $\sigma_i/(\sigma_i^2 + \lambda)$
\end{itemize}

If $\sigma_i$ is small (weak direction):
\begin{equation}
\frac{\sigma_i}{\sigma_i^2 + \lambda} \approx \frac{\sigma_i}{\lambda} \to 0 \text{ as } \sigma_i \to 0
\end{equation}

Ridge \textbf{suppresses weak directions} (directions with small singular values), reducing sensitivity to noise.

\textbf{Geometric interpretation}:

Ridge is equivalent to constrained optimization:
\begin{align}
\text{minimize} \quad & ||y - X\theta||^2 \\
\text{subject to} \quad & ||\theta||^2 \leq t
\end{align}

The constraint $||\theta||^2 \leq t$ defines a sphere in parameter space. The solution is the point on the sphere closest to the unconstrained optimum.

\textbf{Bayesian interpretation}:

Ridge regression = maximum a posteriori (MAP) estimate with Gaussian prior on weights.

Assume:
\begin{itemize}
    \item Likelihood: $y \mid X, \theta \sim \mathcal{N}(X\theta, \sigma^2 I)$
    \item Prior: $\theta \sim \mathcal{N}(0, \tau^2 I)$
\end{itemize}

Then:
\begin{align}
\text{posterior} &\propto \text{likelihood} \times \text{prior} \\
p(\theta \mid y, X) &\propto \exp(-(1/2\sigma^2)||y - X\theta||^2) \cdot \exp(-(1/2\tau^2)||\theta||^2)
\end{align}

Taking negative log:
\begin{equation}
-\log p(\theta \mid y, X) \propto (1/2\sigma^2)||y - X\theta||^2 + (1/2\tau^2)||\theta||^2
\end{equation}

This is exactly Ridge with $\lambda = \sigma^2/\tau^2$.

\textbf{Interpretation}: The prior says ``I believe weights should be close to zero unless the data strongly suggests otherwise.'' This encodes Occam's Razor.

\subsubsection{L1 Regularization (Lasso): Sparsity and Feature Selection}

\textbf{Objective}:
\begin{align}
L_{\text{Lasso}}(\theta) &= \sum_{i} (y_i - \theta^\top x_i)^2 + \lambda||\theta||_1 \\
&= ||y - X\theta||^2 + \lambda\sum_{j} |\theta_j|
\end{align}

\textbf{Key difference from L2}: The L1 norm $||\theta||_1 = \sum|\theta_j|$ is not differentiable at zero.

\textbf{Why L1 produces sparsity}:

\textbf{Geometric argument}:

Lasso is equivalent to:
\begin{align}
\text{minimize} \quad & ||y - X\theta||^2 \\
\text{subject to} \quad & ||\theta||_1 \leq t
\end{align}

The constraint $||\theta||_1 \leq t$ defines a diamond (L1 ball) in 2D, octahedron in 3D, cross-polytope in high dimensions.

Key property: \textbf{Has corners at the axes} (e.g., points like $[t, 0]$, $[0, t]$).

When the level sets of $||y - X\theta||^2$ (ellipses) intersect the L1 ball, they're likely to hit a corner, where some coordinates are exactly zero.

Compare to L2 ball (sphere): smooth, no corners $\rightarrow$ intersection rarely has zero coordinates.

\textbf{Mathematical proof of sparsity} (soft-thresholding):

For simple case (orthogonal features), Lasso solution has closed form:
\begin{equation}
\theta_j = \text{sign}(\theta_{j,\text{ols}}) \max(|\theta_{j,\text{ols}}| - \lambda, 0)
\end{equation}

This is \textbf{soft-thresholding}:
\begin{itemize}
    \item If $|\theta_{j,\text{ols}}| < \lambda$: set $\theta_j = 0$
    \item If $|\theta_{j,\text{ols}}| > \lambda$: shrink toward zero by $\lambda$
\end{itemize}

\textbf{Effect}: Small coefficients get set to exactly zero $\rightarrow$ feature selection.

\textbf{Bayesian interpretation}:

Lasso = MAP estimate with Laplace (double exponential) prior:
\begin{equation}
p(\theta_j) \propto \exp(-\lambda|\theta_j|)
\end{equation}

Laplace prior has heavy peak at zero $\rightarrow$ encourages sparsity.

\textbf{When to use L1 vs L2}:

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Property} & \textbf{L2 (Ridge)} & \textbf{L1 (Lasso)} \\
\midrule
Solution & All weights non-zero (shrunk) & Some weights exactly zero \\
Feature selection & No & Yes \\
When features correlated & Distributes weight & Picks one, zeros others \\
Computational & Closed-form solution & Requires iterative solver \\
Best for & Dense signal & Sparse signal \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Elastic Net: Combining L1 and L2}

\textbf{Objective}:
\begin{equation}
L_{\text{ElasticNet}}(\theta) = ||y - X\theta||^2 + \lambda_1||\theta||_1 + \lambda_2||\theta||^2
\end{equation}

\textbf{Why combine?}

\begin{enumerate}
    \item \textbf{Grouped selection}: When features are correlated, Lasso picks one arbitrarily. Elastic net encourages selecting all correlated features together (Ridge behavior) while still doing feature selection (Lasso behavior).

    \item \textbf{Stability}: Lasso can be unstable with correlated featuresâ€”small data changes lead to different feature selections. Elastic net is more stable.
\end{enumerate}

\textbf{Typical parameterization}:
\begin{equation}
L = ||y - X\theta||^2 + \lambda(\alpha||\theta||_1 + (1-\alpha)||\theta||^2)
\end{equation}

where $\alpha \in [0, 1]$ controls L1/L2 mix:
\begin{itemize}
    \item $\alpha = 0$: Pure Ridge
    \item $\alpha = 1$: Pure Lasso
    \item $\alpha = 0.5$: Equal mix
\end{itemize}

\subsubsection{Dropout: Stochastic Regularization for Neural Networks}

Dropout (Srivastava et al., 2014) is a different beastâ€”it's regularization via randomness.

\textbf{Algorithm} (training):

For each mini-batch:
\begin{enumerate}
    \item For each neuron in layer $l$ (except output), set activation$_i = 0$ with probability $p$ (typically $p = 0.5$)
    \item Scale remaining activations by $1/(1-p)$
    \item Forward and backward pass as usual
\end{enumerate}

\textbf{At test time}: Use all neurons, no dropout.

\textbf{Why it works}:

\textbf{Ensemble interpretation}:
\begin{itemize}
    \item Each training step uses a different sub-network (different neurons dropped)
    \item Training with dropout $\approx$ training $2^n$ different networks (where $n =$ number of neurons)
    \item At test time, using all neurons $\approx$ ensemble prediction of all sub-networks
\end{itemize}

\textbf{Mathematically}:

Let activation at neuron $j$ in layer $l$ be $a_j$.

\textbf{With dropout}:
\begin{equation}
\tilde{a}_j = r_j \cdot a_j / (1-p)
\end{equation}

where $r_j \sim \text{Bernoulli}(1-p)$ ($r_j = 1$ with probability $1-p$, else 0).

\textbf{Expected value}:
\begin{align}
\mathbb{E}[\tilde{a}_j] &= \mathbb{E}[r_j \cdot a_j / (1-p)] \\
&= \mathbb{E}[r_j] \cdot a_j / (1-p) \\
&= (1-p) \cdot a_j / (1-p) \\
&= a_j
\end{align}

The scaling by $1/(1-p)$ ensures that the expected activation is the same as without dropout.

\textbf{At test time}, we want $\mathbb{E}[\tilde{a}]$, so we just use $a$ (no randomness, no scaling).

\textbf{Why it regularizes}:

\begin{enumerate}
    \item \textbf{Prevents co-adaptation}: Neurons can't rely on specific other neurons (they might be dropped). Forces each neuron to learn robust features.

    \item \textbf{Noise injection}: Adding multiplicative noise to activations has a regularizing effect, similar to adding noise to weights.
\end{enumerate}

\textbf{Connection to L2 regularization} (proven for linear models):

For linear model $y_i = \theta^\top x_i$ with dropout on $x$:
\begin{equation}
\mathbb{E}[\text{loss with dropout}] \approx \text{loss without dropout} + (\lambda/2)||\theta||^2
\end{equation}

So dropout on inputs is approximately L2 regularization on weights!

\textbf{Practical notes}:
\begin{itemize}
    \item Dropout rate $p = 0.5$ is common for hidden layers
    \item Input layer: $p = 0.2$ (lighter dropout)
    \item Output layer: no dropout
    \item Convolutional layers: use lower $p$ (0.1-0.2) or spatial dropout
\end{itemize}

\subsubsection{Early Stopping: Implicit Regularization}

\textbf{Algorithm}:
\begin{enumerate}
    \item Monitor validation loss during training
    \item Stop when validation loss starts increasing (even if training loss keeps decreasing)
\end{enumerate}

\textbf{Why it's regularization}:

\textbf{Bias-variance over time}:
\begin{itemize}
    \item Early training: High bias (model hasn't learned much), low variance
    \item Late training: Low bias (model fits training data), high variance (overfits)
\end{itemize}

Early stopping finds the sweet spot.

\textbf{Mathematical connection to regularization} (Gunter et al., 2020):

For gradient descent on smooth loss, early stopping $\approx$ Tikhonov regularization (L2).

Specifically, stopping at iteration $T$ is equivalent to solving:
\begin{equation}
\text{minimize} \quad L(\theta) + \lambda(T)||\theta - \theta_0||^2
\end{equation}

where $\lambda(T) \propto 1/T$.

More iterations = less regularization. Early stop = stronger regularization.

\subsubsection{Regularization and Generalization: The Theory}

\textbf{Why does regularization help generalization?}

\textbf{Statistical learning theory answer}:

Generalization error has two components:
\begin{equation}
E_{\text{test}} = E_{\text{train}} + (\text{complexity penalty})
\end{equation}

Regularization reduces model complexity, trading off training error for better test error.

\textbf{Rademacher complexity} (measure of model class richness):

Without regularization: High Rademacher complexity $\rightarrow$ can fit noise $\rightarrow$ poor generalization.

With regularization: Restricted function class $\rightarrow$ lower complexity $\rightarrow$ better generalization bounds.

\textbf{Formal theorem} (simplified):

For Ridge regression with regularization $\lambda$:
\begin{equation}
E_{\text{test}} \leq E_{\text{train}} + O\left(\sqrt{\frac{d}{n\lambda}}\right)
\end{equation}

where $d =$ dimensions, $n =$ samples.

Larger $\lambda$ $\rightarrow$ smaller generalization gap.

But also:
\begin{equation}
E_{\text{train}} \text{ increases with } \lambda
\end{equation}

Optimal $\lambda$ balances these.

\subsubsection{Summary: Regularization Methods Comparison}

\begin{table}[h]
\centering
\small
\begin{tabular}{p{2.5cm}p{3.5cm}p{2.5cm}p{3.5cm}}
\toprule
\textbf{Method} & \textbf{How It Works} & \textbf{Effect} & \textbf{When to Use} \\
\midrule
\textbf{L2 (Ridge)} & Penalize $||\theta||^2$ & Shrink all weights toward zero & Dense features, multicollinearity \\
\textbf{L1 (Lasso)} & Penalize $||\theta||_1$ & Set some weights to exactly zero & Feature selection, sparse signals \\
\textbf{Elastic Net} & Combine L1 + L2 & Grouped selection + sparsity & Correlated features with selection \\
\textbf{Dropout} & Randomly drop neurons & Prevent co-adaptation & Neural networks, large models \\
\textbf{Early Stopping} & Stop before convergence & Limit effective model complexity & Any iterative training \\
\textbf{Data Augmentation} & Artificially expand dataset & Forces invariances & Computer vision, limited data \\
\bottomrule
\end{tabular}
\end{table}

\begin{insightbox}
\textbf{Key Insight}: All regularization methods encode a prior belief: ``Simpler models generalize better.'' They differ in how they define ``simple'':
\begin{itemize}
    \item L2: Small weights
    \item L1: Few weights
    \item Dropout: Robust features
    \item Early stopping: Smooth loss landscape
\end{itemize}
\end{insightbox}

\section{Overfitting Disasters in Real Systems}

Overfitting isn't academic. It's a production disaster.

\subsection{War Story: Feature Leakage Causing Fake Accuracy}

\textbf{The Setup}: A startup built a model to predict which leads would convert to paying customers. They had 50 features: company size, industry, engagement metrics, etc.

\textbf{Training}: 95\% accuracy! They celebrated.

\textbf{Deployment}: 55\% accuracy. Barely better than random. The company nearly pivoted away from ML entirely.

\textbf{The Investigation}: They sorted features by importance. Top feature: \texttt{days\_until\_conversion}.

Wait, what?

\textbf{The Bug}: \texttt{days\_until\_conversion} was only defined for leads that \textit{did} convert. For non-converting leads, it was set to -1.

The model learned: \texttt{if days\_until\_conversion != -1, then converts}. Perfect correlation, because the feature was derived from the label.

In production, \texttt{days\_until\_conversion} was unknown (obviously). The feature was missing. The model had no signal.

\textbf{The Lesson}: Overfitting to spurious patterns is easy. The model found the easiest path to high training accuracy, which was a data bug.

\section{Things That Will Confuse You}

\subsection{``My test accuracy is 99\%, ship it!''}
Did you test on a representative distribution? Is the test set too similar to training? Are you overfitting to the test set by tuning hyperparameters?

\subsection{``More features is always better''}
More features = more risk of overfitting. Especially with small data. Sometimes less is more.

\subsection{``Neural networks don't need feature engineering''}
They automate it, but you still need to understand what features matter. Garbage inputs = garbage outputs, even with deep learning.

\subsection{``Regularization is just a trick''}
It's a principled way to encode ``simpler models generalize better'' (Occam's Razor). It's not a hack, it's a philosophy.

\section{Common Traps}

\textbf{Trap \#1: Not using cross-validation}

Single train/test split can be lucky or unlucky. Use k-fold cross-validation to estimate generalization robustly.

\textbf{Trap \#2: Tuning hyperparameters on the test set}

Every time you adjust a parameter based on test performance, you leak test information into your model. Use a validation set.

\textbf{Trap \#3: Ignoring class imbalance}

If 99\% of examples are negative, a model that predicts ``always negative'' gets 99\% accuracy. Use balanced metrics (F1, AUC).

\textbf{Trap \#4: Forgetting about feature scaling}

Linear models and distance-based models (k-NN, SVM) are sensitive to feature scales. Normalize features to $[0,1]$ or standardize to mean=0, std=1.

\section{Production Reality Check}

What actually matters in production:

\begin{itemize}
    \item \textbf{Latency}: Can you serve predictions in $<$10ms?
    \item \textbf{Interpretability}: Can you explain decisions to stakeholders?
    \item \textbf{Robustness}: Does the model degrade gracefully on out-of-distribution inputs?
    \item \textbf{Maintainability}: Can someone else debug this in 6 months?
\end{itemize}

Often, a simple logistic regression beats a complex neural net on these axes.

\section{Build This Mini Project}

\textbf{Goal}: Experience the bias-variance tradeoff viscerally.

\textbf{Task}: Fit polynomials of different degrees to noisy data and watch overfitting/underfitting happen.

Here's complete, runnable code:

\begin{lstlisting}[language=Python]
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error
from sklearn.pipeline import make_pipeline

np.random.seed(42)

# =============================================================================
# Generate Data
# =============================================================================
# True function: sine wave
def true_function(x):
    return np.sin(2 * np.pi * x)

# Training data: 20 points with noise
n_train = 20
x_train = np.linspace(0, 1, n_train)
y_train = true_function(x_train) + np.random.normal(0, 0.3, n_train)

# Test data: 100 points with noise (to evaluate generalization)
n_test = 100
x_test = np.linspace(0, 1, n_test)
y_test = true_function(x_test) + np.random.normal(0, 0.3, n_test)

# Dense x for plotting smooth curves
x_plot = np.linspace(0, 1, 200)

print("="*70)
print("BIAS-VARIANCE TRADEOFF DEMONSTRATION")
print("="*70)
print(f"Training points: {n_train}")
print(f"Test points: {n_test}")
print(f"True function: sin(2Ï€x)")
print(f"Noise level: Ïƒ = 0.3")
print()

# =============================================================================
# Fit Polynomials of Different Degrees
# =============================================================================
degrees = [1, 4, 15]
colors = ['red', 'green', 'orange']
results = {}

print("Model Performance:")
print("-" * 50)
print(f"{'Degree':<10} {'Train MSE':<15} {'Test MSE':<15} {'Status'}")
print("-" * 50)

for degree, color in zip(degrees, colors):
    # Create polynomial regression pipeline
    model = make_pipeline(
        PolynomialFeatures(degree, include_bias=False),
        LinearRegression()
    )

    # Fit on training data
    model.fit(x_train.reshape(-1, 1), y_train)

    # Predict
    y_train_pred = model.predict(x_train.reshape(-1, 1))
    y_test_pred = model.predict(x_test.reshape(-1, 1))
    y_plot_pred = model.predict(x_plot.reshape(-1, 1))

    # Calculate errors
    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)

    # Determine status
    if degree == 1:
        status = "UNDERFIT (high bias)"
    elif degree == 4:
        status = "GOOD FIT âœ“"
    else:
        status = "OVERFIT (high variance)"

    results[degree] = {
        'model': model,
        'train_mse': train_mse,
        'test_mse': test_mse,
        'y_plot': y_plot_pred,
        'color': color,
        'status': status
    }

    print(f"{degree:<10} {train_mse:<15.4f} {test_mse:<15.4f} {status}")

print("-" * 50)

# =============================================================================
# Demonstrate Regularization Fixing Overfitting
# =============================================================================
print("\n" + "="*70)
print("REGULARIZATION: Fixing the Degree-15 Overfit")
print("="*70)

# Degree 15 with L2 regularization (Ridge)
alphas = [0, 0.0001, 0.01, 1.0]

print(f"\n{'Alpha (Î»)':<12} {'Train MSE':<15} {'Test MSE':<15} {'Effect'}")
print("-" * 55)

for alpha in alphas:
    if alpha == 0:
        model = make_pipeline(
            PolynomialFeatures(15, include_bias=False),
            LinearRegression()
        )
        effect = "No regularization (overfit)"
    else:
        model = make_pipeline(
            PolynomialFeatures(15, include_bias=False),
            Ridge(alpha=alpha)
        )
        if alpha == 0.0001:
            effect = "Light regularization"
        elif alpha == 0.01:
            effect = "Good regularization âœ“"
        else:
            effect = "Too much (underfit)"

    model.fit(x_train.reshape(-1, 1), y_train)

    train_mse = mean_squared_error(y_train,
                                   model.predict(x_train.reshape(-1, 1)))
    test_mse = mean_squared_error(y_test,
                                  model.predict(x_test.reshape(-1, 1)))

    print(f"{alpha:<12} {train_mse:<15.4f} {test_mse:<15.4f} {effect}")

print("-" * 55)

# Save visualization
plt.savefig('bias_variance_tradeoff.png', dpi=150, bbox_inches='tight')
print("\nðŸ“Š Visualization saved as 'bias_variance_tradeoff.png'")
\end{lstlisting}

\textbf{Expected Output}:
\begin{lstlisting}[style=outputstyle]
======================================================================
BIAS-VARIANCE TRADEOFF DEMONSTRATION
======================================================================
Training points: 20
Test points: 100
True function: sin(2Ï€x)
Noise level: Ïƒ = 0.3

Model Performance:
--------------------------------------------------
Degree     Train MSE       Test MSE        Status
--------------------------------------------------
1          0.4523          0.4892          UNDERFIT (high bias)
4          0.0734          0.1124          GOOD FIT âœ“
15         0.0312          0.5765          OVERFIT (high variance)
--------------------------------------------------

======================================================================
REGULARIZATION: Fixing the Degree-15 Overfit
======================================================================

Alpha (Î»)    Train MSE       Test MSE        Effect
-------------------------------------------------------
0            0.0312          0.5765          No regularization (overfit)
0.0001       0.0456          0.2341          Light regularization
0.01         0.0812          0.1198          Good regularization âœ“
1.0          0.3234          0.3567          Too much (underfit)
-------------------------------------------------------

ðŸ“Š Visualization saved as 'bias_variance_tradeoff.png'
\end{lstlisting}

\textbf{What This Demonstrates}:

\begin{enumerate}
    \item \textbf{The U-shaped test error curve}: As complexity increases, test error first decreases (reducing bias), then increases (increasing variance)

    \item \textbf{The gap between train and test error}: Large gap = overfitting. The model memorized training data but can't generalize.

    \item \textbf{Regularization as a fix}: L2 regularization (Ridge) shrinks weights, effectively reducing model complexity even with high-degree polynomials.
\end{enumerate}

\begin{insightbox}
\textbf{Key Insight}: Model complexity must match data complexity. Too simple = can't capture pattern. Too complex = captures noise as pattern. Regularization lets you use complex models while controlling overfitting.
\end{insightbox}

\section{Statistical Learning Theory: Why Generalization is Possible}

The fundamental question of machine learning: \textbf{Why do models trained on finite data generalize to unseen data?}

This section provides the mathematical foundations explaining when and why generalization works.

\subsection{The Learning Problem (Formally)}

\textbf{Setup}:
\begin{itemize}
    \item Unknown data distribution: $P(X, Y)$
    \item Training set: $S = \{(x_1, y_1), \ldots, (x_n, y_n)\}$ drawn i.i.d. from $P$
    \item Hypothesis class: $\mathcal{H} = \{h: X \to Y\}$ (set of possible models)
    \item Learning algorithm: $A: S \to h \in \mathcal{H}$
\end{itemize}

\textbf{Goal}: Find $h$ such that:
\begin{equation}
\text{True risk (generalization error): } R(h) = \mathbb{E}_{(x,y)\sim P}[\text{loss}(h(x), y)]
\end{equation}

is minimized.

\textbf{Problem}: We only have access to:
\begin{equation}
\text{Empirical risk (training error): } \hat{R}(h) = \frac{1}{n} \sum_{i=1}^{n} \text{loss}(h(x_i), y_i)
\end{equation}

\textbf{Question}: When does $\hat{R}(h) \approx R(h)$? When can we trust training error as a proxy for test error?

\subsection{PAC Learning: Probably Approximately Correct}

\textbf{Definition} (Valiant 1984):

A hypothesis class $\mathcal{H}$ is \textbf{PAC learnable} if there exists an algorithm $A$ and polynomial function $m(\cdot,\cdot,\cdot,\cdot)$ such that:

For any distribution $P$, any $\varepsilon > 0$, any $\delta > 0$, with probability at least $1-\delta$ over samples $S$ of size $n \geq m(1/\varepsilon, 1/\delta, \text{size}(x), \text{size}(h))$:
\begin{equation}
R(h) \leq \min_{h^*\in\mathcal{H}} R(h^*) + \varepsilon
\end{equation}

\textbf{Translation}:
\begin{itemize}
    \item \textbf{Probably} ($1-\delta$): With high probability over random training sets
    \item \textbf{Approximately} ($\varepsilon$): Get close to the best possible $h$ in our class
    \item \textbf{Correct}: Output has low true error
\end{itemize}

\textbf{What this means}:
\begin{enumerate}
    \item We can't guarantee finding the absolute best hypothesis
    \item But we can get close (within $\varepsilon$)
    \item With high confidence ($1-\delta$)
    \item Using polynomial amount of data/computation
\end{enumerate}

\subsection{VC Dimension: Measuring Hypothesis Class Complexity}

\textbf{Shattering}: A set of points $\{x_1, \ldots, x_m\}$ is \textbf{shattered} by $\mathcal{H}$ if for every possible labeling $\{y_1, \ldots, y_m\} \in \{-1,+1\}^m$, there exists $h \in \mathcal{H}$ that perfectly classifies those points.

\textbf{VC Dimension}: The largest number of points that can be shattered by $\mathcal{H}$.

\textbf{Formal definition}:
\begin{equation}
\text{VC}(\mathcal{H}) = \max\{m : \exists x_1,\ldots,x_m \text{ that can be shattered by } \mathcal{H}\}
\end{equation}

\textbf{Examples}:

\begin{enumerate}
    \item \textbf{Linear classifiers in 2D}:
    \begin{itemize}
        \item VC dimension = 3
        \item Any 3 points (not collinear) can be shattered
        \item But not all 4 points can be shattered (XOR problem)
    \end{itemize}

    \item \textbf{Linear classifiers in $d$ dimensions}:
    \begin{itemize}
        \item $\text{VC}(\text{linear}) = d + 1$
        \item More parameters $\rightarrow$ higher VC dimension $\rightarrow$ more complex
    \end{itemize}

    \item \textbf{Neural network with $W$ weights}:
    \begin{itemize}
        \item $\text{VC}(\text{network}) = O(W \log W)$
        \item Massive networks have huge VC dimension
    \end{itemize}
\end{enumerate}

\textbf{Why VC dimension matters}:

\textbf{Fundamental Theorem of Statistical Learning} (Vapnik-Chervonenkis):

For binary classification, $\mathcal{H}$ is PAC learnable if and only if $\text{VC}(\mathcal{H}) < \infty$.

Moreover, sample complexity (number of samples needed) is:
\begin{equation}
n = O\left(\frac{d}{\varepsilon^2} \log\left(\frac{1}{\delta}\right)\right)
\end{equation}

where $d = \text{VC}(\mathcal{H})$.

\textbf{Generalization bound}:

With probability at least $1-\delta$:
\begin{equation}
R(h) \leq \hat{R}(h) + O\left(\sqrt{\frac{d \log(n/d) + \log(1/\delta)}{n}}\right)
\end{equation}

\textbf{Interpretation}:
\begin{itemize}
    \item Higher VC dimension $\rightarrow$ larger generalization gap
    \item More samples $\rightarrow$ smaller generalization gap
    \item True error = training error + complexity penalty
\end{itemize}

\subsection{The Bias-Complexity Tradeoff (Formal Version)}

\textbf{Decomposition of expected error}:

For a learning algorithm producing $\hat{h}$:
\begin{equation}
\mathbb{E}[R(\hat{h})] = \text{Approximation error} + \text{Estimation error}
\end{equation}

\textbf{Approximation error}: How well can best $h^* \in \mathcal{H}$ represent truth?
\begin{equation}
\text{Approx} = \min_{h\in\mathcal{H}} R(h)
\end{equation}

\textbf{Estimation error}: How much worse is $\hat{h}$ than $h^*$?
\begin{equation}
\text{Estim} = \mathbb{E}[R(\hat{h})] - \min_{h\in\mathcal{H}} R(h)
\end{equation}

\textbf{Tradeoff}:
\begin{itemize}
    \item \textbf{Small $\mathcal{H}$} (low VC dimension):
    \begin{itemize}
        \item Low estimation error (few samples suffice)
        \item High approximation error (can't represent complex functions)
    \end{itemize}

    \item \textbf{Large $\mathcal{H}$} (high VC dimension):
    \begin{itemize}
        \item High estimation error (need many samples)
        \item Low approximation error (can represent complex functions)
    \end{itemize}
\end{itemize}

\textbf{Optimal $\mathcal{H}$ balances both}.

\subsection{The Curse of Dimensionality}

\textbf{Problem}: In high dimensions, data becomes sparse.

\textbf{Example}: Unit hypercube $[0,1]^d$

To cover 10\% of each dimension with $\varepsilon$-ball, need:
\begin{equation}
\text{Number of balls} = (1/\varepsilon)^d
\end{equation}

For $d=10$, $\varepsilon=0.1$: Need $10^{10}$ balls.

For $d=100$, $\varepsilon=0.1$: Need $10^{100}$ balls (more than atoms in universe).

\textbf{Consequence}: Uniform convergence requires exponentially many samples in high dimensions.

\textbf{Why machine learning still works}:

\begin{enumerate}
    \item \textbf{Data lies on low-dimensional manifolds}:
    \begin{itemize}
        \item Images don't uniformly fill $256^3$ space
        \item They lie on a much lower-dimensional manifold
        \item Intrinsic dimension $\ll$ ambient dimension
    \end{itemize}

    \item \textbf{Smoothness assumptions}:
    \begin{itemize}
        \item Similar inputs $\rightarrow$ similar outputs
        \item Don't need to sample everywhere, just enough to interpolate
    \end{itemize}

    \item \textbf{Inductive biases in models}:
    \begin{itemize}
        \item CNNs assume locality and translation invariance
        \item These structural assumptions massively reduce effective hypothesis class size
    \end{itemize}
\end{enumerate}

\subsection{No Free Lunch Theorem}

\textbf{Theorem} (Wolpert \& Macready 1997):

Averaged over all possible data distributions, all learning algorithms have identical performance.

\textbf{Formal statement}:

For any two algorithms $A_1$ and $A_2$:
\begin{equation}
\mathbb{E}_P [R(A_1)] = \mathbb{E}_P [R(A_2)]
\end{equation}

where expectation is over all possible distributions $P$.

\textbf{Implication}: There is no universally best learning algorithm.

\textbf{Why this matters}:

Machine learning works because:
\begin{enumerate}
    \item We're not interested in ``all possible distributions''
    \item Real-world distributions have structure
    \item We design algorithms with \textbf{inductive biases} matching real-world structure
\end{enumerate}

\textbf{Example}:
\begin{itemize}
    \item Images have spatial locality $\rightarrow$ CNNs work well
    \item Text has sequential structure $\rightarrow$ RNNs/Transformers work well
    \item These wouldn't work on truly random data
\end{itemize}

\begin{insightbox}
\textbf{The lesson}: Success in ML comes from making good assumptions about the data distribution.
\end{insightbox}

\subsection{Why Deep Learning Breaks Classical Theory}

\textbf{Paradox}: Modern deep networks have:
\begin{itemize}
    \item VC dimension $\gg$ number of samples
    \item Can fit random labels perfectly (zero training error on noise)
    \item Yet generalize well on real data
\end{itemize}

Classical theory predicts: ``This should overfit catastrophically.''

\textbf{Reality}: Deep networks generalize.

\textbf{Explanations} (active research):

\begin{enumerate}
    \item \textbf{Implicit regularization of SGD}:
    \begin{itemize}
        \item SGD biases toward simple (low-norm, large-margin) solutions
        \item Not all functions in hypothesis class are equally likely under SGD
    \end{itemize}

    \item \textbf{Data-dependent bounds}:
    \begin{itemize}
        \item Classical bounds use worst-case VC dimension
        \item Real data lives on low-dimensional manifolds
        \item Effective hypothesis class is much smaller
    \end{itemize}

    \item \textbf{Optimization vs generalization decoupling}:
    \begin{itemize}
        \item Classical theory: Hard to optimize $\rightarrow$ hard to overfit
        \item Deep learning: Easy to optimize (overparameterized), but still generalizes
        \item Different regime requires new theory
    \end{itemize}

    \item \textbf{Compression perspective}:
    \begin{itemize}
        \item Networks that generalize can be compressed (pruned, quantized)
        \item Effective number of parameters $\ll$ actual parameters
        \item Generalization depends on effective complexity, not parameter count
    \end{itemize}
\end{enumerate}

\textbf{Current state}: Theory is catching up. We understand some pieces, but not the complete picture.

\subsection{Summary: When and Why Generalization Works}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Concept} & \textbf{What It Tells Us} \\
\midrule
\textbf{PAC Learning} & Finite VC dimension $\rightarrow$ can learn with polynomial samples \\
\textbf{VC Dimension} & Measures worst-case complexity of hypothesis class \\
\textbf{Rademacher Complexity} & Data-dependent complexity measure \\
\textbf{Margin Theory} & Large margins $\rightarrow$ better generalization \\
\textbf{Curse of Dimensionality} & Need exponential samples for uniform coverage \\
\textbf{No Free Lunch} & Must make assumptions about data distribution \\
\textbf{Occam's Razor} & Simpler hypotheses generalize better \\
\bottomrule
\end{tabular}
\end{table}

\textbf{The Big Picture}:

Machine learning works when:
\begin{enumerate}
    \item \textbf{Data has structure} (not random)
    \item \textbf{Model class contains good approximations} (representational capacity)
    \item \textbf{Sample complexity is manageable} (enough data for VC dimension)
    \item \textbf{Optimization finds good solutions} (tractable training)
    \item \textbf{Inductive biases match problem} (right architecture for task)
\end{enumerate}

When any of these fail, machine learning fails.

The art of machine learning is:
\begin{itemize}
    \item Choosing hypothesis classes with the right complexity
    \item Incorporating appropriate inductive biases
    \item Getting enough data
    \item Using optimization that finds generalizable solutions
\end{itemize}

Theory provides guardrails. Practice involves navigating the tradeoffs.
